{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9baea40",
   "metadata": {},
   "source": [
    "# üß† VectorDB Retrieval-Augmented Generation (RAG) Notebook\n",
    "\n",
    "This notebook demonstrates how to build a local, persistent Vector Database (using ChromaDB) for semantic search and question answering over a collection of documents in various formats (`.pdf`, `.md`, `.txt`). It leverages local models from Ollama for both embedding and generation, making the entire workflow private and offline-friendly.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal is to support Retrieval-Augmented Generation (RAG) using LangChain with:\n",
    "1. **Flexible ingestion** of documents from multiple file formats.\n",
    "2. **Efficient updates** to the VectorDB when new documents are added.\n",
    "3. **Persistent storage**, so data does not need to be reprocessed in every session.\n",
    "4. **Interactive Q&A**, allowing natural language queries over the content.\n",
    "\n",
    "## Design Choice\n",
    "\n",
    "I considered converting all document types to Markdown using [`markitdown`](https://github.com/microsoft/markitdown) for a unified format. However, we chose **not** to take that path to preserve the **fidelity and structure** of original formats like PDFs, which often contain critical layout and semantic cues that can be lost in conversion. Instead, we use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2770431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "#!pip install --upgrade pip --quiet\n",
    "#!pip install --upgrade langchain langchain-ollama langchain-chroma chromadb pymupdf \"unstructured[local-inference]\" ollama --quiet\n",
    "\n",
    "# Required Ollama models - ensure these are installed:\n",
    "# ollama pull gemma3\n",
    "# ollama pull embeddinggemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de79a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prevent Chroma telemetry issues\n",
    "os.environ[\"CHROMADB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Define directories\n",
    "DATA_DIR = \"./data\"\n",
    "CHROMA_DIR = \"./chromadb_store\"\n",
    "\n",
    "SUPPORTED_FORMATS = {\n",
    "    \"pdfs\": \"PyMuPDFLoader\",\n",
    "    \"markdowns\": \"UnstructuredMarkdownLoader\",\n",
    "    \"txt\": \"TextLoader\",\n",
    "}\n",
    "\n",
    "EMBEDDING_MODEL = \"embeddinggemma\"\n",
    "LLM_MODEL = \"gemma3\"\n",
    "\n",
    "\n",
    "# --- Utility: Write Permission Check ---\n",
    "def assert_directory_writable(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    test_file = os.path.join(path, \"test_write.tmp\")\n",
    "    try:\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        print(f\"‚úÖ Confirmed write access to: {path}\")\n",
    "    except Exception as e:\n",
    "        raise PermissionError(f\"‚ùå Cannot write to {path}: {e}\")\n",
    "\n",
    "\n",
    "# --- Document Loaders ---\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "\n",
    "\n",
    "def load_documents(base_dir, formats_dict):\n",
    "    glob_patterns = {\n",
    "        PyMuPDFLoader: \"*.pdf\",\n",
    "        UnstructuredMarkdownLoader: \"*.md\",\n",
    "        TextLoader: \"*.txt\",\n",
    "    }\n",
    "    loaders = {\n",
    "        \"PyMuPDFLoader\": PyMuPDFLoader,\n",
    "        \"UnstructuredMarkdownLoader\": UnstructuredMarkdownLoader,\n",
    "        \"TextLoader\": TextLoader,\n",
    "    }\n",
    "    docs = []\n",
    "    for subdir, loader_name in formats_dict.items():\n",
    "        dir_path = os.path.join(base_dir, subdir)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        loader_cls = loaders[loader_name]\n",
    "        pattern = glob_patterns[loader_cls]\n",
    "        loader = DirectoryLoader(\n",
    "            dir_path,\n",
    "            glob=pattern,\n",
    "            loader_cls=loader_cls,\n",
    "            show_progress=True,\n",
    "        )\n",
    "        loaded = loader.load()\n",
    "        print(f\"üìÇ Loaded {len(loaded)} from {dir_path}\")\n",
    "        if loaded:\n",
    "            print(f\"üìù Sample:\\n{loaded[0].page_content[:500]}\")\n",
    "        docs.extend(loaded)\n",
    "    print(f\"üìÑ Total docs loaded: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# --- Text Splitting ---\n",
    "def split_documents(docs):\n",
    "    # Smaller chunks to keep embeddings safe for Ollama 0.13.x\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,        # was 1000\n",
    "        chunk_overlap=100,     # was 200\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"üî™ {len(chunks)} chunks created\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Vector Store Build ---\n",
    "def build_vectordb(docs, embedding_model, persist_dir):\n",
    "    import shutil\n",
    "    assert_directory_writable(persist_dir)\n",
    "    chunks = split_documents(docs)\n",
    "    try:\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=persist_dir,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"Database error\" in str(e) or \"readonly\" in str(e):\n",
    "            print(f\"‚ö†Ô∏è  Corrupted database, recreating...\")\n",
    "            shutil.rmtree(persist_dir, ignore_errors=True)\n",
    "            os.makedirs(persist_dir, exist_ok=True)\n",
    "            vectordb = Chroma.from_documents(\n",
    "                documents=chunks,\n",
    "                embedding=embedding_model,\n",
    "                persist_directory=persist_dir,\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "    print(f\"‚úÖ VectorDB built at: {persist_dir}\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# --- Vector Store Load ---\n",
    "def load_vectordb(persist_dir, embedding_model):\n",
    "    if not os.path.exists(persist_dir):\n",
    "        raise FileNotFoundError(f\"‚ùå VectorDB directory not found: {persist_dir}. Run with build=True first.\")\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding_model,\n",
    "    )\n",
    "    print(f\"‚úÖ VectorDB loaded from: {persist_dir}\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# --- QA Chain Setup ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_template = \"\"\"\n",
    "You are a helpful assistant answering questions based on the provided context.\n",
    "Use the following pieces of context to answer the user's question. If unsure, state you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(qa_template)\n",
    "\n",
    "\n",
    "def create_qa_chain(vectordb):\n",
    "    try:\n",
    "        llm = OllamaLLM(\n",
    "            model=LLM_MODEL,\n",
    "            base_url=\"http://localhost:11434\",  # make endpoint explicit\n",
    "        )\n",
    "        # Test LLM model\n",
    "        llm.invoke(\"test\")\n",
    "        print(f\"ü§ñ LLM ready: {LLM_MODEL}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"‚ùå LLM model '{LLM_MODEL}' not available. \"\n",
    "            f\"Run: ollama pull {LLM_MODEL}. Error: {e}\"\n",
    "        )\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Build Pipeline ---\n",
    "def initialize_pipeline(build=True):\n",
    "    \"\"\"\n",
    "    Initializes the RAG pipeline using ChromaDB and Ollama.\n",
    "\n",
    "    Parameters:\n",
    "        build (bool): If True, (re)build the vector database from the document source.\n",
    "                      If False, load the existing persisted vector store.\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: A ready-to-use QA chain for question answering.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Initializing pipeline...\")\n",
    "\n",
    "    # Validate Ollama embedding model is available\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            num_ctx=2048,   # keep context size modest and explicit\n",
    "            # keep_alive can be added as seconds (int) if you like, e.g. keep_alive=300\n",
    "        )\n",
    "        # Test embedding model\n",
    "        embedding_model.embed_query(\"test\")\n",
    "        print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL}' is ready\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"‚ùå Embedding model '{EMBEDDING_MODEL}' not available. \"\n",
    "            f\"Run: ollama pull {EMBEDDING_MODEL}. Error: {e}\"\n",
    "        )\n",
    "\n",
    "    if build or not os.path.exists(os.path.join(CHROMA_DIR, \"chroma.sqlite3\")):\n",
    "        docs = load_documents(DATA_DIR, SUPPORTED_FORMATS)\n",
    "        vectordb = build_vectordb(docs, embedding_model, CHROMA_DIR)\n",
    "    else:\n",
    "        vectordb = load_vectordb(CHROMA_DIR, embedding_model)\n",
    "\n",
    "    qa_chain = create_qa_chain(vectordb)\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e612",
   "metadata": {},
   "source": [
    "## (re)build the vector database from the document source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fd9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing pipeline...\n",
      "‚úÖ Embedding model 'embeddinggemma' is ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 406 from ./data/pdfs\n",
      "üìù Sample:\n",
      "ETSI TS 103 994-1 V1.1.1 (2024-03) \n",
      "Cyber Security (CYBER);  \n",
      "Privileged Access Workstations; \n",
      "Part 1: Physical Device \n",
      " \n",
      " \n",
      " \n",
      "TECHNICAL SPECIFICATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 8 from ./data/markdowns\n",
      "üìù Sample:\n",
      "Python Networking Expansion\n",
      "\n",
      "Reference source files: ClientPython.py, ServerPython.py\n",
      "\n",
      "This tutorial will complete the functionality of the client, making it usable (when paired with an equally functional server, which will be written later) for real-world capabilities by adding the networking and logic code necessary for it to exchange query and log data with a server. This will also allow us to explore additional features and nuances of networking in Python.\n",
      "\n",
      "Using SSL to secure connections\n",
      "\n",
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 0 from ./data/txt\n",
      "üìÑ Total docs loaded: 414\n",
      "‚úÖ Confirmed write access to: ./chromadb_store\n",
      "üî™ 3400 chunks created\n",
      "‚úÖ VectorDB built at: ./chromadb_store\n",
      "ü§ñ LLM ready: gemma3\n"
     ]
    }
   ],
   "source": [
    "qa_chain = initialize_pipeline(\n",
    "    build=True\n",
    ")  # Use build=True to (re)build the vector database from the document source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21e28e",
   "metadata": {},
   "source": [
    "## Load the existing persisted vector store & ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64058cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing pipeline...\n",
      "‚úÖ Embedding model 'embeddinggemma' is ready\n",
      "‚úÖ VectorDB loaded from: ./chromadb_store\n",
      "ü§ñ LLM ready: gemma3\n",
      "\n",
      "‚ùì Question: Why are pickles bad?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "I don't know. The provided context is about computer science terms like locks, singletons, nested functions, and endianness. It doesn't contain information about why pickles might be bad."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "Tutorial-4-MultipleConnections.md, Page N/A\n",
      "\n",
      "‚ùì Question: What is a ClickFix?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "ClickFix is a tactic that involves deceiving users into downloading and running malware on their machines. It‚Äôs used to distribute malware like remote access trojans and infostealers, bypassing web browser security features. It‚Äôs been used in campaigns involving compromised websites and phishing emails, starting in early March 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "clickfix-attacks-sector-alert-tlpclear.pdf, Page 0\n",
      "\n",
      "‚ùì Question: What is LummaC2?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "LummaC2 is an (infostealer) malware that is able to infiltrate victim computer networks and exfiltrate sensitive information. It has been associated with infections from November 2023 through May 2025 and has been observed targeting vulnerable individuals‚Äô and organizations‚Äô computer networks across multiple U.S. critical infrastructure sectors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 1\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 7\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "qa_chain = initialize_pipeline(\n",
    "    build=False\n",
    ")  # Use build=False to load the existing persisted vector store\n",
    "\n",
    "\n",
    "# Ask a question\n",
    "def ask(question):\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    display(Markdown(f\"**Answer:**\\n\\n{response['result']}\"))\n",
    "\n",
    "    print(\"\\nüìÑ Sources:\")\n",
    "    seen_sources = set()\n",
    "    for doc in response[\"source_documents\"][:2]:\n",
    "        source_info = f\"{os.path.basename(doc.metadata.get('source', 'unknown'))}, Page {doc.metadata.get('page', 'N/A')}\"\n",
    "        if source_info not in seen_sources:\n",
    "            print(source_info)\n",
    "            seen_sources.add(source_info)\n",
    "\n",
    "ask(\"Why are pickles bad?\")\n",
    "ask(\"What is a ClickFix?\")\n",
    "ask(\"What is LummaC2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbb7d9",
   "metadata": {},
   "source": [
    "## Pro Tip\n",
    "Reuse the same qa_chain object throughout the notebook without needing to reload or rebuild it ‚Äî unless you've added new documents to the vector store.\n",
    "\n",
    "If you did add new files, you'd need to:\n",
    "1. Re-load and split documents.\n",
    "2. Re-embed and persist them.\n",
    "3. Optionally reinitialize the qa_chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d79a1d",
   "metadata": {},
   "source": [
    "### Option B: Load Persisted Vector Store and Setup QA Chain\n",
    "This cell loads the saved Chroma vector store and sets up a new QA chain using Ollama for LLM and embedding. No re-indexing or file reprocessing is done here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148d2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "# Set env for Chroma (just in case)\n",
    "os.environ[\"CHROMADB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Config\n",
    "CHROMA_DIR = \"./chromadb_store\"\n",
    "EMBEDDING_MODEL = \"embeddinggemma\"\n",
    "LLM_MODEL = \"gemma3\"\n",
    "\n",
    "# Prompt template\n",
    "qa_template = \"\"\"\n",
    "You are a helpful assistant answering questions based on the provided context.\n",
    "Use the following pieces of context to answer the user's question. If unsure, state you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(qa_template)\n",
    "\n",
    "# Reload vector DB + embedding\n",
    "embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Recreate QA chain\n",
    "llm = OllamaLLM(model=LLM_MODEL)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "\n",
    "# Ask a question\n",
    "def ask(question):\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    display(Markdown(f\"**Answer:**\\n\\n{response['result']}\"))\n",
    "\n",
    "    print(\"\\nüìÑ Sources:\")\n",
    "    seen_sources = set()\n",
    "    for doc in response[\"source_documents\"][:2]:\n",
    "        source_info = f\"{os.path.basename(doc.metadata.get('source', 'unknown'))}, Page {doc.metadata.get('page', 'N/A')}\"\n",
    "        if source_info not in seen_sources:\n",
    "            print(source_info)\n",
    "            seen_sources.add(source_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfcf6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: What is a ClickFix?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "ClickFix is a tactic that involves deceiving users into downloading and running malware on their machines. It utilizes compromised websites and phishing emails to distribute malware like remote access trojans and infostealers. It bypasses web browser security features and appears less suspicious to users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "clickfix-attacks-sector-alert-tlpclear.pdf, Page 0\n",
      "\n",
      "‚ùì Question: What is LummaC2?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "LummaC2 is an (infostealer) malware that is able to infiltrate victim computer networks and exfiltrate sensitive information. It has been associated with infections from November 2023 through May 2025 and has been observed threatening vulnerable individuals‚Äô and organizations‚Äô computer networks across multiple U.S. critical infrastructure sectors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 1\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 7\n",
      "\n",
      "‚ùì Question: Why is the sky blue?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "I don‚Äôt know. The context provided discusses concepts like endianness, locks, certificate verification, and singletons, none of which relate to the color of the sky."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "Tutorial-4-MultipleConnections.md, Page N/A\n",
      "Tutorial-7-PythonNetworkingExpansion.md, Page N/A\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is a ClickFix?\")\n",
    "ask(\"What is LummaC2?\")\n",
    "ask(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fd0b4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline using:\n",
    "- **ChromaDB** for persistent vector storage\n",
    "- **Ollama** for local LLM and embedding models\n",
    "- **LangChain** for document processing and QA chains\n",
    "\n",
    "Key features:\n",
    "- Supports multiple document formats (PDF, Markdown, Text)\n",
    "- Persistent storage - no need to rebuild unless adding new documents\n",
    "- Local execution - no external API calls required\n",
    "- Error handling for missing models\n",
    "\n",
    "To add new documents:\n",
    "1. Place files in the appropriate `./data/` subdirectory\n",
    "2. Run `initialize_pipeline(build=True)` to rebuild the vector store\n",
    "3. Use the updated QA chain for queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
