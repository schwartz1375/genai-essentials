{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9baea40",
   "metadata": {},
   "source": [
    "# üß† VectorDB Retrieval-Augmented Generation (RAG) Notebook\n",
    "\n",
    "This notebook demonstrates how to build a local, persistent Vector Database (using ChromaDB) for semantic search and question answering over a collection of documents in various formats (`.pdf`, `.md`, `.txt`). It leverages local models from Ollama for both embedding and generation, making the entire workflow private and offline-friendly.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The goal is to support Retrieval-Augmented Generation (RAG) using LangChain with:\n",
    "1. **Flexible ingestion** of documents from multiple file formats.\n",
    "2. **Efficient updates** to the VectorDB when new documents are added.\n",
    "3. **Persistent storage**, so data does not need to be reprocessed in every session.\n",
    "4. **Interactive Q&A**, allowing natural language queries over the content.\n",
    "\n",
    "## Design Choice\n",
    "\n",
    "I considered converting all document types to Markdown using [`markitdown`](https://github.com/microsoft/markitdown) for a unified format. However, we chose **not** to take that path to preserve the **fidelity and structure** of original formats like PDFs, which often contain critical layout and semantic cues that can be lost in conversion. Instead, we use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2770431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "#!pip install --upgrade pip --quiet\n",
    "#!pip install --upgrade langchain langchain-ollama langchain-chroma chromadb pymupdf \"unstructured[local-inference]\" ollama --quiet\n",
    "\n",
    "# Required Ollama models - ensure these are installed:\n",
    "# ollama pull gemma3\n",
    "# ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de79a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Prevent Chroma telemetry issues\n",
    "os.environ[\"CHROMADB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Define directories\n",
    "DATA_DIR = \"./data\"\n",
    "CHROMA_DIR = \"./chromadb_store\"\n",
    "\n",
    "SUPPORTED_FORMATS = {\n",
    "    \"pdfs\": \"PyMuPDFLoader\",\n",
    "    \"markdowns\": \"UnstructuredMarkdownLoader\",\n",
    "    \"txt\": \"TextLoader\",\n",
    "}\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"gemma3\"\n",
    "\n",
    "\n",
    "# --- Utility: Write Permission Check ---\n",
    "def assert_directory_writable(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    test_file = os.path.join(path, \"test_write.tmp\")\n",
    "    try:\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        print(f\"‚úÖ Confirmed write access to: {path}\")\n",
    "    except Exception as e:\n",
    "        raise PermissionError(f\"‚ùå Cannot write to {path}: {e}\")\n",
    "\n",
    "\n",
    "# --- Document Loaders ---\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "\n",
    "\n",
    "def load_documents(base_dir, formats_dict):\n",
    "    glob_patterns = {\n",
    "        PyMuPDFLoader: \"*.pdf\",\n",
    "        UnstructuredMarkdownLoader: \"*.md\",\n",
    "        TextLoader: \"*.txt\",\n",
    "    }\n",
    "    loaders = {\n",
    "        \"PyMuPDFLoader\": PyMuPDFLoader,\n",
    "        \"UnstructuredMarkdownLoader\": UnstructuredMarkdownLoader,\n",
    "        \"TextLoader\": TextLoader,\n",
    "    }\n",
    "    docs = []\n",
    "    for subdir, loader_name in formats_dict.items():\n",
    "        dir_path = os.path.join(base_dir, subdir)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        loader_cls = loaders[loader_name]\n",
    "        pattern = glob_patterns[loader_cls]\n",
    "        loader = DirectoryLoader(\n",
    "            dir_path, glob=pattern, loader_cls=loader_cls, show_progress=True\n",
    "        )\n",
    "        loaded = loader.load()\n",
    "        print(f\"üìÇ Loaded {len(loaded)} from {dir_path}\")\n",
    "        if loaded:\n",
    "            print(f\"üìù Sample:\\n{loaded[0].page_content[:500]}\")\n",
    "        docs.extend(loaded)\n",
    "    print(f\"üìÑ Total docs loaded: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# --- Text Splitting ---\n",
    "def split_documents(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"üî™ {len(chunks)} chunks created\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Vector Store Build ---\n",
    "def build_vectordb(docs, embedding_model, persist_dir):\n",
    "    assert_directory_writable(persist_dir)\n",
    "    chunks = split_documents(docs)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks, embedding=embedding_model, persist_directory=persist_dir\n",
    "    )\n",
    "    print(f\"‚úÖ VectorDB built at: {persist_dir}\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# --- Vector Store Load ---\n",
    "def load_vectordb(persist_dir, embedding_model):\n",
    "    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_model)\n",
    "    print(f\"‚úÖ VectorDB loaded from: {persist_dir}\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# --- QA Chain Setup ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_template = \"\"\"\n",
    "You are a helpful assistant answering questions based on the provided context.\n",
    "Use the following pieces of context to answer the user's question. If unsure, state you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(qa_template)\n",
    "\n",
    "\n",
    "def create_qa_chain(vectordb):\n",
    "    try:\n",
    "        llm = OllamaLLM(model=LLM_MODEL)\n",
    "        # Test LLM model\n",
    "        llm.invoke(\"test\")\n",
    "        print(f\"ü§ñ LLM ready: {LLM_MODEL}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå LLM model '{LLM_MODEL}' not available. Run: ollama pull {LLM_MODEL}. Error: {e}\")\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Build Pipeline ---\n",
    "def initialize_pipeline(build=True):\n",
    "    \"\"\"\n",
    "    Initializes the RAG pipeline using ChromaDB and Ollama.\n",
    "\n",
    "    Parameters:\n",
    "        build (bool): If True, (re)build the vector database from the document source.\n",
    "                      If False, load the existing persisted vector store.\n",
    "\n",
    "    Returns:\n",
    "        RetrievalQA: A ready-to-use QA chain for question answering.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Initializing pipeline...\")\n",
    "    \n",
    "    # Validate Ollama models are available\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "        # Test embedding model\n",
    "        embedding_model.embed_query(\"test\")\n",
    "        print(f\"‚úÖ Embedding model '{EMBEDDING_MODEL}' is ready\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå Embedding model '{EMBEDDING_MODEL}' not available. Run: ollama pull {EMBEDDING_MODEL}. Error: {e}\")\n",
    "\n",
    "    if build or not os.path.exists(os.path.join(CHROMA_DIR, \"chroma.sqlite3\")):\n",
    "        docs = load_documents(DATA_DIR, SUPPORTED_FORMATS)\n",
    "        vectordb = build_vectordb(docs, embedding_model, CHROMA_DIR)\n",
    "    else:\n",
    "        vectordb = load_vectordb(CHROMA_DIR, embedding_model)\n",
    "\n",
    "    qa_chain = create_qa_chain(vectordb)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e612",
   "metadata": {},
   "source": [
    "## (re)build the vector database from the document source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fd9668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing pipeline...\n",
      "‚úÖ Embedding model 'nomic-embed-text' is ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 406 from ./data/pdfs\n",
      "üìù Sample:\n",
      "ETSI TS 103 994-1 V1.1.1 (2024-03) \n",
      "Cyber Security (CYBER);  \n",
      "Privileged Access Workstations; \n",
      "Part 1: Physical Device \n",
      " \n",
      " \n",
      " \n",
      "TECHNICAL SPECIFICATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:11<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 8 from ./data/markdowns\n",
      "üìù Sample:\n",
      "Python Networking Expansion\n",
      "\n",
      "Reference source files: ClientPython.py, ServerPython.py\n",
      "\n",
      "This tutorial will complete the functionality of the client, making it usable (when paired with an equally functional server, which will be written later) for real-world capabilities by adding the networking and logic code necessary for it to exchange query and log data with a server. This will also allow us to explore additional features and nuances of networking in Python.\n",
      "\n",
      "Using SSL to secure connections\n",
      "\n",
      "B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 0 from ./data/txt\n",
      "üìÑ Total docs loaded: 414\n",
      "‚úÖ Confirmed write access to: ./chromadb_store\n",
      "üî™ 1730 chunks created\n",
      "‚úÖ VectorDB built at: ./chromadb_store\n",
      "ü§ñ LLM ready: gemma3\n"
     ]
    }
   ],
   "source": [
    "qa_chain = initialize_pipeline(\n",
    "    build=True\n",
    ")  # Use build=True to (re)build the vector database from the document source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21e28e",
   "metadata": {},
   "source": [
    "## Load the existing persisted vector store & ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64058cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing pipeline...\n",
      "‚úÖ Embedding model 'nomic-embed-text' is ready\n",
      "‚úÖ VectorDB loaded from: ./chromadb_store\n",
      "ü§ñ LLM ready: gemma3\n",
      "\n",
      "‚ùì Question: Why are pickles bad?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "I understand you're asking a question, but the provided context doesn't contain any information about pickles or why they might be bad. It focuses on a software product‚Äôs security."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "E02781980_Telecommunications_Security_CoP_Accessible.pdf, Page 120\n",
      "\n",
      "‚ùì Question: What is a ClickFix?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "The ClickFix tactic deceives users into downloading and running malware on their machines without them knowing. Threat actors initiate these campaigns by logging into websites with stolen credentials and installing fake plugins in compromised environments. Once installed, the plugins inject malicious JavaScript containing fake browser update malware that uses blockchain and smart contracts to obtain malicious payloads. When executed in the browser, JavaScript presents users with fake browser update notifications that guide them to install malware on their computer (usually remote access trojans and various infostealers like Vidar Stealer, DarkGate, and Lumma Stealer)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "clickfix-attacks-sector-alert-tlpclear.pdf, Page 0\n",
      "\n",
      "‚ùì Question: What is LummaC2?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "LummaC2.exe is a file that, upon execution, enters a main routine with four sub-routines. The first routine decrypts strings for a message box displayed to the user."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "qa_chain = initialize_pipeline(\n",
    "    build=False\n",
    ")  # Use build=False to load the existing persisted vector store\n",
    "\n",
    "\n",
    "# Ask a question\n",
    "def ask(question):\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    display(Markdown(f\"**Answer:**\\n\\n{response['result']}\"))\n",
    "\n",
    "    print(\"\\nüìÑ Sources:\")\n",
    "    seen_sources = set()\n",
    "    for doc in response[\"source_documents\"][:2]:\n",
    "        source_info = f\"{os.path.basename(doc.metadata.get('source', 'unknown'))}, Page {doc.metadata.get('page', 'N/A')}\"\n",
    "        if source_info not in seen_sources:\n",
    "            print(source_info)\n",
    "            seen_sources.add(source_info)\n",
    "\n",
    "ask(\"Why are pickles bad?\")\n",
    "ask(\"What is a ClickFix?\")\n",
    "ask(\"What is LummaC2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbb7d9",
   "metadata": {},
   "source": [
    "## Pro Tip\n",
    "Reuse the same qa_chain object throughout the notebook without needing to reload or rebuild it ‚Äî unless you've added new documents to the vector store.\n",
    "\n",
    "If you did add new files, you'd need to:\n",
    "1. Re-load and split documents.\n",
    "2. Re-embed and persist them.\n",
    "3. Optionally reinitialize the qa_chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d79a1d",
   "metadata": {},
   "source": [
    "### Option B: Load Persisted Vector Store and Setup QA Chain\n",
    "This cell loads the saved Chroma vector store and sets up a new QA chain using Ollama for LLM and embedding. No re-indexing or file reprocessing is done here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148d2858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "# Set env for Chroma (just in case)\n",
    "os.environ[\"CHROMADB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Config\n",
    "CHROMA_DIR = \"./chromadb_store\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"gemma3\"\n",
    "\n",
    "# Prompt template\n",
    "qa_template = \"\"\"\n",
    "You are a helpful assistant answering questions based on the provided context.\n",
    "Use the following pieces of context to answer the user's question. If unsure, state you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(qa_template)\n",
    "\n",
    "# Reload vector DB + embedding\n",
    "embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)\n",
    "\n",
    "# Recreate QA chain\n",
    "llm = OllamaLLM(model=LLM_MODEL)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "\n",
    "# Ask a question\n",
    "def ask(question):\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    display(Markdown(f\"**Answer:**\\n\\n{response['result']}\"))\n",
    "\n",
    "    print(\"\\nüìÑ Sources:\")\n",
    "    seen_sources = set()\n",
    "    for doc in response[\"source_documents\"][:2]:\n",
    "        source_info = f\"{os.path.basename(doc.metadata.get('source', 'unknown'))}, Page {doc.metadata.get('page', 'N/A')}\"\n",
    "        if source_info not in seen_sources:\n",
    "            print(source_info)\n",
    "            seen_sources.add(source_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfcf6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: What is a ClickFix?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "The ClickFix tactic deceives users into downloading and running malware on their machines without realizing it. Threat actors initiate these campaigns by logging into websites with stolen credentials and installing fake plugins in compromised environments. Once installed, the plugins inject malicious JavaScript containing fake browser update malware that uses blockchain and smart contracts to obtain malicious payloads. When executed in the browser, JavaScript presents users with fake browser update notifications that guide them to install malware."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "clickfix-attacks-sector-alert-tlpclear.pdf, Page 0\n",
      "\n",
      "‚ùì Question: What is LummaC2?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "LummaC2.exe is a file that, upon execution, enters a main routine with four sub-routines. The first routine decrypts strings for a message box displayed to the user."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "aa25-141b-threat-actors-deploy-lummac2-malware-to-exfiltrate-sensitive-data-from-organizations.pdf, Page 1\n",
      "\n",
      "‚ùì Question: Why is the sky blue?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "I do not know. The provided context discusses cyber threats, geopolitical monitoring, and security intelligence ‚Äì it does not contain information about why the sky is blue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Sources:\n",
      "CTA-RU-2024-0530.pdf, Page 2\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is a ClickFix?\")\n",
    "ask(\"What is LummaC2?\")\n",
    "ask(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fd0b4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline using:\n",
    "- **ChromaDB** for persistent vector storage\n",
    "- **Ollama** for local LLM and embedding models\n",
    "- **LangChain** for document processing and QA chains\n",
    "\n",
    "Key features:\n",
    "- Supports multiple document formats (PDF, Markdown, Text)\n",
    "- Persistent storage - no need to rebuild unless adding new documents\n",
    "- Local execution - no external API calls required\n",
    "- Error handling for missing models\n",
    "\n",
    "To add new documents:\n",
    "1. Place files in the appropriate `./data/` subdirectory\n",
    "2. Run `initialize_pipeline(build=True)` to rebuild the vector store\n",
    "3. Use the updated QA chain for queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
