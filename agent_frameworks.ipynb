{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Frameworks: LangChain Patterns with Local Models\n",
    "\n",
    "This notebook explores agent architecture patterns using **LangChain** with local Ollama models. While there are multiple agent frameworks available (AWS Strands, CrewAI, AutoGen, LlamaIndex), we focus on LangChain because:\n",
    "\n",
    "- **Mature ecosystem** with extensive documentation and community support\n",
    "- **Local model compatibility** works seamlessly with Ollama\n",
    "- **Flexible patterns** supports multiple agent architectures\n",
    "- **Production ready** with robust error handling and monitoring\n",
    "- **Consistency** with other notebooks in this collection\n",
    "\n",
    "## Core Agent Concepts Covered:\n",
    "1. **Agent Architecture Patterns** (ReAct, Plan-and-Execute, Multi-agent)\n",
    "2. **Tool Integration and Function Calling**\n",
    "3. **Memory Management and State Persistence** \n",
    "4. **Agent Orchestration and Workflow Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langchain-ollama langchain-community\n",
    "# Ensure Ollama models are available:\n",
    "# ollama pull gemma3\n",
    "# ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.tools import Tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Setup Ollama LLM\n",
    "llm = ChatOllama(model=\"gemma3\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple ReAct Agent\n",
    "\n",
    "ReAct (Reasoning + Acting) pattern combines reasoning and action in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Invalid expression\"\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Calculator\", func=calculator, description=\"Calculate math expressions\"),\n",
    "    Tool(name=\"Search\", func=search.run, description=\"Search the internet\")\n",
    "]\n",
    "\n",
    "# Create ReAct agent\n",
    "react_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mOkay, let's break this down. First, I need to calculate 25 * 4. Then, I need to find the latest news about AI.\n",
      "Action: Calculator\n",
      "Action Input: \"25 * 4\"\u001b[0m\u001b[36;1m\u001b[1;3m100\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=latest%20news%20about%20AI): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mOkay, I have the result of the calculation. Now I need to find the latest news about AI.\n",
      "Action: Search\n",
      "Action Input: \"latest news about AI\"\u001b[0m\u001b[33;1m\u001b[1;3mAI News reports on the latest artificial intelligence news and insights. Explore industry trends from the frontline of AI . It’s my favorite AI story as of late , probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to. As the writer of the story, Laurie Clarke, points out, it’s not a total pipe dream that AI could be therapeutically useful. Snowflake (SNOW) also ticked up following rival Databricks' latest $100 billion private valuation and news that its AI products now exceed a $1 billion revenue run-rate. Meanwhile, Oracle advanced ahead of its earnings release, with Wall Street raising price targets in anticipation of... The Trump administration, in its recent AI Action Plan, seemed to signal that Grok’s offensiveness might not be a problem — it included an anti-“woke AI ” order that largely aligns with Musk’s politics, and xAI’s latest DoD contract was awarded after the MechaHitler incident. That’s actually the fun part about AI as a technology, it provides a general-purpose way of interacting with systems and getting intelligent answers. But then product managers come in and ask: How do I take this technology and turn it into a capability my users can benefit from?\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mFinal Answer: 25 * 4 = 100. The latest news about AI includes reports on Snowflake's stock performance, Databricks' valuation, the Trump administration's AI Action Plan, Grok's offensiveness, and xAI's contract with the DoD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: 25 * 4 = 100. The latest news about AI includes reports on Snowflake's stock performance, Databricks' valuation, the Trump administration's AI Action Plan, Grok's offensiveness, and xAI's contract with the DoD.\n"
     ]
    }
   ],
   "source": [
    "# Test ReAct agent\n",
    "question = \"What is 25 * 4 and what's the latest news about AI?\"\n",
    "result = agent_executor.invoke({\"input\": question})\n",
    "print(f\"Result: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Management and State Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First interaction:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=My%20name%20is%20John): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAction: Search\n",
      "Action Input: \"My name is John\"\u001b[0m\u001b[33;1m\u001b[1;3mTracia Traill My name is John Taylor \"the Bass God \" True Wild Boys story. My dad kept telling me to turn the stereo down. Intro to Wild Boys started I ... __ ABOUT Mynameisjohn is John Lillis, an artist, DJ and record collector living in Liscannor, Co. Clare. John has over 20 years experience DJing around ... “My name is John Cena .” “The name I use in the ring is John Cena.” “My government name is John Cena.” John Cena was born to be a wrestler and says he'd do it ... The song was written while Duran Duran were on tour , the music was written by the band, and lyrics by Simon Le Bon. The story behind the lyrics was told by both ... To edit the info that you use on Google services, like your name and photo, sign in to your account. You can also choose what personal info to show when you interact with others on Google services.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI found multiple mentions of \"John\" in the search results. It seems like \"John\" is a common name.\n",
      "Final Answer: John\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: John\n",
      "\n",
      "Second interaction:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=What%27s%20my%20name%3F): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find out John's name. I can use the Search tool for this.\n",
      "Action: Search\n",
      "Action Input: \"What's my name?\"\u001b[0m\u001b[33;1m\u001b[1;3mFeaturing guest vocals from Canadian rapper Drake, the song was released as the second single from Loud on October 25, 2010, through Def Jam Recordings and in Germany on January 21, 2011, as a single through Universal Music Group. Kicking off in San Diego, CA on July 17, the arena tour will celebrate the chart-topping music from Disney’ s “Descendants” and “Zombies” movie franchises, inspiring families, and fans of all ages... Oct 26, 2010 · A song by Rihanna featuring Drake about their lovers and their names. The lyrics include romantic bars, sexual innuendos, and a catchy chorus of \"oh na-na, what's my name?\" Featuring guest vocals from Canadian rapper Drake, the song was released as the second single from Loud on October 25, 2010, through Def Jam Recordings and in Germany on January 21, 2011, as a single through Universal Music Group. Rihanna \" What ' s My Name ?\": Oh na-na, what ' s my name ? Oh na-na, what 's my na... A song by Rihanna featuring Drake, released in 2010 as the second single from her album Loud. The lyrics are about sexual intercourse and romance, and the song topped the charts in several countries. What is 'Your Name'? Your Name is a beautiful work entirely composed of hand-drawn frames . It combines life-like vitality with a traditional anime style, resulting in staging, setting, apparel, and other features that frequently leave the viewer in a state of awe. What is Know my Name about? Know My Name is an intimate, profoundly moving memoir that exposes a patriarchal culture biased to protect perpetrators , a criminal justice system designed to fail the most vulnerable, and ultimately shines with the courage required to move through suffering and live a full and beautiful life. Who wrote the song 'what's my name'? The dancehall -infused electro - R&B song was produced by the Norwegian production duo Stargate, and was written by the duo along with Ester Dean, Traci Hale, and Drake . Lyrically, it incorporates themes of romance and sex. \"What's My Name?\" Does Rihanna perform 'what's my name' on 'the seven'? On MTV' s 'The Seven,' Live In Times Square\" MTV Buzzworthy Blog\". MTV. November 15, 2010. Archived from the original on November 17, 2010. Retrieved December 21, 2010. ^ Wete, Brad (November 17, 2010). \"Rihanna performs ' What ' s My Name ' on the Late Show with David Letterman\". Music-mix.ew.com. Archived from the original on November 20, 2010. So clearly while they were having sex she was screaming his name . And afterwards he texts her this as a flirty little message to remind her of how much she enjoyed it. Oct 26, 2010 · A song by Rihanna featuring Drake about their lovers and their names. The lyrics include romantic bars, sexual innuendos, and a catchy chorus of \"oh na-na, what's my name?\" Rihanna \" What ' s My Name ?\": Oh na-na, what ' s my name ? Oh na-na, what 's my na... So clearly while they were having sex she was screaming his name . And afterwards he texts her this as a flirty little message to remind her of how much she enjoyed it.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=What%27s%20my%20name%3F): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find out John's name. The text explicitly states \"My name is John\".\n",
      "Action: Search\n",
      "Action Input: \"What's my name?\"\u001b[0m\u001b[33;1m\u001b[1;3mMusic video by China Anne McClain, Thomas Doherty, Dylan Playfair performing What ' s My Name (From \"Descendants 2\"/Sing-Along).© 2020 Walt Disney Records. Practice saying, \" What ' s your name ?\" and \" My name is ....\" with this fun and catchy song! Download a free mp3 of this song here: Dream English song by Matt R... Nothin' stays the same But I'm still in the game What ' s my name ? (Ringo!) Spinning round and round You broke the speed of sound Tearin' up the freeway With the tumblin' of the dice And... Ooh na na, what ’ s my name Ooh na na, what ’ s my name . hello what is your name song for kids.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mFinal Answer: My name is John.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: My name is John.\n"
     ]
    }
   ],
   "source": [
    "# Add memory to agent with proper prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create memory-aware prompt template\n",
    "memory_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "# Use string-based memory instead of message-based\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "\n",
    "# Create agent with memory-aware prompt\n",
    "memory_agent = create_react_agent(llm, tools, memory_prompt)\n",
    "agent_with_memory = AgentExecutor(\n",
    "    agent=memory_agent, \n",
    "    tools=tools, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test conversation memory\n",
    "print(\"First interaction:\")\n",
    "result1 = agent_with_memory.invoke({\"input\": \"My name is John\"})\n",
    "print(f\"Response: {result1['output']}\")\n",
    "\n",
    "print(\"\\nSecond interaction:\")\n",
    "result2 = agent_with_memory.invoke({\"input\": \"What's my name?\"})\n",
    "print(f\"Response: {result2['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plan-and-Execute Pattern\n",
    "\n",
    "This pattern first creates a plan, then executes each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan:\n",
      "Okay, here’s a breakdown of the task into simple, numbered steps:\n",
      "\n",
      "1. **Find the Current Bitcoin Price:**\n",
      "   * Use a reputable cryptocurrency exchange or financial website to find the current price of Bitcoin (BTC) in your preferred currency (e.g., USD, EUR, GBP).  Good sources include:\n",
      "      * CoinMarketCap: [https://coinmarketcap.com/](https://coinmarketcap.com/)\n",
      "      * Coinbase: [https://www.coinbase.com/](https://www.coinbase.com/)\n",
      "      * Binance: [https://www.binance.com/](https://www.binance.com/)\n",
      "      * Google Finance (search \"Bitcoin price\")\n",
      "\n",
      "2. **Calculate the Cost of 10 Bitcoin:**\n",
      "   * Multiply the current Bitcoin price by 10.  For example, if Bitcoin is trading at $65,000, then 10 Bitcoin would cost $650,000.\n",
      "\n",
      "3. **Search for Recent Cryptocurrency Regulation News:**\n",
      "   * Use a search engine (Google, Bing, etc.) to search for “recent cryptocurrency regulation news”.\n",
      "   * Focus on news from reputable sources like:\n",
      "      * Reuters: [https://www.reuters.com/](https://www.reuters.com/)\n",
      "      * Bloomberg: [https://www.bloomberg.com/](https://www.bloomberg.com/)\n",
      "      * The Wall Street Journal: [https://www.wsj.com/](https://www.wsj.com/)\n",
      "      * CoinDesk: [https://www.coindesk.com/](https://www.coindesk.com/)\n",
      "      *  Look for articles published within the last week or two.\n",
      "\n",
      "4. **Summarize Your Findings:**\n",
      "   *  Write down the current Bitcoin price.\n",
      "   *  Write down the calculated cost of 10 Bitcoin.\n",
      "   *  Briefly summarize the key points from the cryptocurrency regulation news you found.\n",
      "\n",
      "\n",
      "\n",
      "Do you want me to help you with any of these steps, such as finding the current price or suggesting search terms?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=current%20bitcoin%20price): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mOkay, let's execute this plan step by step. I will start by finding the current Bitcoin price.\n",
      "Action: Search\n",
      "Action Input: \"current bitcoin price\"\u001b[0m\u001b[33;1m\u001b[1;3mOct 24, 2024 — The live Bitcoin price today is $ 112324.69 USD with a 24-hour trading volume of $48471501125.52 USD. We update our BTC to USD price in real-time. Aug 4, 2025 — Price of BTC is $ 112,092.16 right now . Looking at Bitcoin's historical prices, it's 3% up from the previous week's price of $108,864.82, and observed a 1% ... Jan 24, 2025 — The price of Bitcoin has decreased today. 1 BTC currently costs £83,247.95 , which represents a change of +0.53% in the last hour and a change of -0.07% in the ... 2 hours ago — Bitcoin has a current supply of 19,918,781. The last known price of Bitcoin is 111,646.55091185 USD and is down -0.31 over the last 24 hours. It is currently ... 4 days ago — Data is currently not available. Key Data. Daily Session High. 113260. Daily Session Low. 111121. Last Value . 112713. Net Change. 657.8. Percent Change. +0.59%.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mOkay, I have gathered the current Bitcoin price. It's $112,713. I will now proceed to calculate the cost of 10 Bitcoin and search for recent cryptocurrency regulation news.\n",
      "Action: Calculator\n",
      "Action Input: 112713 * 10\u001b[0m\u001b[36;1m\u001b[1;3m1127130\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=recent%20cryptocurrency%20regulation%20news): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Now that I have the current Bitcoin price ($112,713) and the cost of 10 Bitcoin ($1,127,130), I will proceed to search for recent cryptocurrency regulation news.\n",
      "Action: Search\n",
      "Action Input: \"recent cryptocurrency regulation news\"\u001b[0m\u001b[33;1m\u001b[1;3m5 days ago · Crypto regulation has advanced: The Fed set an Oct. 21 stablecoin focus. The SEC and CFTC have allowed spot trading on registered exchanges. 5 days ago · Keep up with the latest crypto regulation . Dive into the latest news and updates to stay informed, maximize opportunities, and drive success in the crypto world. Aug 28, 2025 · Crypto Regulation news and analysis. Be Informed of the latest trends and events around regulator bodies and crypto assets. Aug 20, 2025 · From updates on regulatory frameworks in different countries to analysis of how government policies affect the crypto market, readers can stay informed on all aspects of the regulatory landscape. 5 days ago · Lawmakers in the Senate Banking Committee have a new draft of the crypto market structure bill that would establish U.S. regulations for crypto trading.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mFinal Answer: The current Bitcoin price is $112,713. The cost of 10 Bitcoin is $1,127,130. Recent cryptocurrency regulation news includes updates on the Fed's stablecoin focus, the SEC and CFTC allowing spot trading on registered exchanges, and a new draft of the crypto market structure bill being considered by lawmakers in the Senate Banking Committee.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Plan-and-Execute Result: The current Bitcoin price is $112,713. The cost of 10 Bitcoin is $1,127,130. Recent cryptocurrency regulation news includes updates on the Fed's stablecoin focus, the SEC and CFTC allowing spot trading on registered exchanges, and a new draft of the crypto market structure bill being considered by lawmakers in the Senate Banking Committee.\n"
     ]
    }
   ],
   "source": [
    "# Simple Plan-and-Execute implementation\n",
    "def plan_and_execute(task: str, llm, tools):\n",
    "    \"\"\"Simple plan-and-execute pattern\"\"\"\n",
    "    \n",
    "    # Step 1: Create a plan\n",
    "    planning_prompt = f\"\"\"Break down this task into simple steps:\n",
    "    Task: {task}\n",
    "    \n",
    "    Provide a numbered list of steps.\"\"\"\n",
    "    \n",
    "    plan_response = llm.invoke(planning_prompt)\n",
    "    plan = plan_response.content\n",
    "    print(f\"Plan:\\n{plan}\\n\")\n",
    "    \n",
    "    # Step 2: Execute with agent\n",
    "    agent = create_react_agent(llm, tools, react_prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    \n",
    "    execution_prompt = f\"\"\"Execute this plan step by step:\n",
    "    {plan}\n",
    "    \n",
    "    Original task: {task}\"\"\"\n",
    "    \n",
    "    result = executor.invoke({\"input\": execution_prompt})\n",
    "    return result['output']\n",
    "\n",
    "# Test plan-and-execute\n",
    "complex_task = \"Research the current price of Bitcoin, calculate what 10 Bitcoin would cost, and find recent news about cryptocurrency regulation\"\n",
    "result = plan_and_execute(complex_task, llm, tools)\n",
    "print(f\"Plan-and-Execute Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Agent Orchestration\n",
    "\n",
    "Create specialized agents for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=current%20population%20of%20Tokyo): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=Tokyo%20population%20area): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=Tokyo%20population%20area): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=8%2C300%2C000%20/%202194): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=8%2C300%2C000%20/%202194): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Agent Result:\n",
      "Research: 8,300,000 / 2194 = 380.26 people per square kilometer.\n",
      "\n",
      "Calculations: 3783.0446672743847\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents\n",
    "research_tools = [Tool(name=\"Search\", func=search.run, description=\"Research information\")]\n",
    "math_tools = [Tool(name=\"Calculator\", func=calculator, description=\"Perform calculations\")]\n",
    "\n",
    "research_agent = create_react_agent(llm, research_tools, react_prompt)\n",
    "math_agent = create_react_agent(llm, math_tools, react_prompt)\n",
    "\n",
    "research_executor = AgentExecutor(agent=research_agent, tools=research_tools)\n",
    "math_executor = AgentExecutor(agent=math_agent, tools=math_tools)\n",
    "\n",
    "# Orchestrate agents\n",
    "def orchestrate_agents(task: str):\n",
    "    # Step 1: Research\n",
    "    research_result = research_executor.invoke({\"input\": f\"Research: {task}\"})\n",
    "    \n",
    "    # Step 2: Extract numbers and calculate\n",
    "    math_task = f\"Based on this research: {research_result['output']}, perform any necessary calculations\"\n",
    "    math_result = math_executor.invoke({\"input\": math_task})\n",
    "    \n",
    "    return f\"Research: {research_result['output']}\\n\\nCalculations: {math_result['output']}\"\n",
    "\n",
    "multi_agent_task = \"Find the current population of Tokyo and calculate how many people that is per square kilometer if Tokyo is 2194 km²\"\n",
    "result = orchestrate_agents(multi_agent_task)\n",
    "print(f\"Multi-Agent Result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Tool Integration with State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding tasks:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result: Tasks: Review code, Write tests\n"
     ]
    }
   ],
   "source": [
    "# Stateful tool example\n",
    "class TaskTracker:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "    \n",
    "    def add_task(self, task: str) -> str:\n",
    "        self.tasks.append(task)\n",
    "        return f\"Added task: {task}. Total tasks: {len(self.tasks)}\"\n",
    "    \n",
    "    def list_tasks(self, _: str = \"\") -> str:  # Accept dummy argument\n",
    "        if not self.tasks:\n",
    "            return \"No tasks found\"\n",
    "        return \"Tasks: \" + \", \".join(self.tasks)\n",
    "\n",
    "tracker = TaskTracker()\n",
    "\n",
    "stateful_tools = [\n",
    "    Tool(name=\"AddTask\", func=tracker.add_task, description=\"Add a task to track\"),\n",
    "    Tool(name=\"ListTasks\", func=tracker.list_tasks, description=\"List all tasks\")\n",
    "]\n",
    "\n",
    "stateful_agent = create_react_agent(llm, stateful_tools, react_prompt)\n",
    "stateful_executor = AgentExecutor(agent=stateful_agent, tools=stateful_tools)\n",
    "\n",
    "# Test stateful operations\n",
    "print(\"Adding tasks:\")\n",
    "result1 = stateful_executor.invoke({\"input\": \"Add task: Review code\"})\n",
    "result2 = stateful_executor.invoke({\"input\": \"Add task: Write tests\"})\n",
    "result3 = stateful_executor.invoke({\"input\": \"List all my tasks\"})\n",
    "print(f\"Final result: {result3['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG-Enabled Agent\n",
    "\n",
    "Building on the local_rag.ipynb concepts with Ollama embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to search the documents for Linux commands and then calculate the total count.\n",
      "Action: DocumentSearch\n",
      "Action Input: \"Linux commands\"\u001b[0m\u001b[36;1m\u001b[1;3mclass SERVER_COMMANDS:\n",
      "    UPLOAD_CMD = 0x0001\n",
      "    QUERY_CMD = 0x0002\n",
      "\n",
      "Since we only have two types of data we are transmitting to the server, we only need to define two commands here, but we could easily expand the protocol by defining more later. Each is given a numeric value, which will be the 2-byte value that is actually transmitted over the network to determine the use case. We could define a larger size to allow for larger numbers of commands, or use a 1-byte value (or even just one single bit that was combined with other data like the size of the transmission) to minimize bandwidth usage, since we only have two commands, but this is a good starting point.\n",
      "\n",
      "class SERVER_COMMANDS:\n",
      "    UPLOAD_CMD = 0x0001\n",
      "    QUERY_CMD = 0x0002\n",
      "\n",
      "Since we only have two types of data we are transmitting to the server, we only need to define two commands here, but we could easily expand the protocol by defining more later. Each is given a numeric value, which will be the 2-byte value that is actually transmitted over the network to determine the use case. We could define a larger size to allow for larger numbers of commands, or use a 1-byte value (or even just one single bit that was combined with other data like the size of the transmission) to minimize bandwidth usage, since we only have two commands, but this is a good starting point.\n",
      "\n",
      "class SERVER_COMMANDS:\n",
      "    UPLOAD_CMD = 0x0001\n",
      "    QUERY_CMD = 0x0002\n",
      "\n",
      "Since we only have two types of data we are transmitting to the server, we only need to define two commands here, but we could easily expand the protocol by defining more later. Each is given a numeric value, which will be the 2-byte value that is actually transmitted over the network to determine the use case. We could define a larger size to allow for larger numbers of commands, or use a 1-byte value (or even just one single bit that was combined with other data like the size of the transmission) to minimize bandwidth usage, since we only have two commands, but this is a good starting point.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mFinal Answer: The documents mention two Linux commands: UPLOAD_CMD and QUERY_CMD. The total count is 2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RAG Agent Result: The documents mention two Linux commands: UPLOAD_CMD and QUERY_CMD. The total count is 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load and process documents\n",
    "loader = DirectoryLoader('./data/markdowns/', glob=\"*.md\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_agents\")\n",
    "\n",
    "# Create RAG tool\n",
    "def rag_search(query: str) -> str:\n",
    "    \"\"\"Search through local documents\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_tools = [\n",
    "    Tool(name=\"DocumentSearch\", func=rag_search, description=\"Search through local documents\"),\n",
    "    Tool(name=\"Calculator\", func=calculator, description=\"Calculate math expressions\")\n",
    "]\n",
    "\n",
    "rag_agent = create_react_agent(llm, rag_tools, react_prompt)\n",
    "rag_executor = AgentExecutor(agent=rag_agent, tools=rag_tools, verbose=True)\n",
    "\n",
    "# Test RAG agent\n",
    "rag_query = \"What Linux commands are mentioned in the documents and how would you calculate their total count?\"\n",
    "result = rag_executor.invoke({\"input\": rag_query})\n",
    "print(f\"RAG Agent Result: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Considerations\n",
    "\n",
    "When deploying agents to production, you need proper error handling, logging, and monitoring. Key considerations include:\n",
    "\n",
    "- **Error Handling**: Graceful failure when tools fail or parsing errors occur\n",
    "- **Logging**: Track agent actions, decisions, and performance\n",
    "- **Timeouts**: Prevent infinite loops or long-running operations\n",
    "- **Rate Limiting**: Control API usage and costs\n",
    "- **Monitoring**: Track success rates, response times, and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Agent Action: Calculator - 2+2\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Agent Finished: {'output': '4'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production result: 4\n"
     ]
    }
   ],
   "source": [
    "# Production configuration with error handling and logging\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ProductionCallbackHandler(BaseCallbackHandler):\n",
    "    def on_agent_action(self, action, **kwargs):\n",
    "        logger.info(f\"Agent Action: {action.tool} - {action.tool_input}\")\n",
    "    \n",
    "    def on_agent_finish(self, finish, **kwargs):\n",
    "        logger.info(f\"Agent Finished: {finish.return_values}\")\n",
    "    \n",
    "    def on_chain_error(self, error, **kwargs):\n",
    "        logger.error(f\"Agent Error: {error}\")\n",
    "\n",
    "production_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    callbacks=[ProductionCallbackHandler()],\n",
    "    max_iterations=10,\n",
    "    early_stopping_method=\"generate\",\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Test production setup\n",
    "result = production_executor.invoke({\"input\": \"What is 2+2?\"})\n",
    "print(f\"Production result: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Agent Pattern: Workflow Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ddgs.ddgs:Error in engine wikipedia: DDGSException(\"RuntimeError: RuntimeError('error sending request for url (https://wt.wikipedia.org/w/api.php?action=opensearch&profile=fuzzy&limit=1&search=current%20Bitcoin%20price%20USD): client error (Connect)\\\\n\\\\nCaused by:\\\\n    0: client error (Connect)\\\\n    1: dns error: failed to lookup address information: nodename nor servname provided, or not known\\\\n    2: failed to lookup address information: nodename nor servname provided, or not known')\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing step_1: Search for current Bitcoin price\n",
      "Executing step_2: Calculate cost of 5 Bitcoin\n",
      "\n",
      "Workflow Results:\n",
      "step_1: {'description': 'Search for current Bitcoin price', 'result': 'We update our BTC to USD price in real-time.The current valuation of Bitcoin is constantly moving, all day every day. It is a truly global asset. From a start of under one cent per coin, BTC has risen in price by thousands of percent to the numbers you see above. Aug 4, 2025 — Get the latest price, news, live charts, and market trends about Bitcoin. The current price of Bitcoin in United States is $ 112092.16 per (BTC / USD ) 2 hours ago — US Dollar, Vietnamese Dong. Convert. Conversion Value. 0.854409 . Top Stories On CoinDesk Bitcoin Price Index (XBX). Read full story. Cryptocurrencies Bitcoin, ... Aug 20, 2025 — BTCUSD Bitcoin US DollarCurrency Exchange Rate Live Price Chart. Chart; Quotes; Financials; Alerts. Exchange Rate. 112298 . Daily Change. 767.0000 0.69%. Monthly. 5 hours ago — Bitcoin fell 1.3% to $ 111,576.5 by 02:04 ET (06:04 GMT). Major crypto stocks, including Circle Internet Group Inc (NYSE:CRCL) and Bullish ...'}\n",
      "step_2: {'description': 'Calculate cost of 5 Bitcoin', 'result': '250000'}\n"
     ]
    }
   ],
   "source": [
    "# Custom workflow agent that manages multi-step processes\n",
    "class WorkflowAgent:\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "        self.workflow_state = {}\n",
    "    \n",
    "    def execute_workflow(self, steps: list) -> dict:\n",
    "        results = {}\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            step_name = f\"step_{i+1}\"\n",
    "            print(f\"Executing {step_name}: {step['description']}\")\n",
    "            \n",
    "            if step['tool'] in self.tools:\n",
    "                tool = self.tools[step['tool']]\n",
    "                result = tool.func(step['input'])\n",
    "                results[step_name] = {\n",
    "                    'description': step['description'],\n",
    "                    'result': result\n",
    "                }\n",
    "                # Store result for next steps\n",
    "                self.workflow_state[step_name] = result\n",
    "            else:\n",
    "                results[step_name] = {'error': f\"Tool {step['tool']} not found\"}\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create workflow agent\n",
    "workflow_agent = WorkflowAgent(llm, tools)\n",
    "\n",
    "# Define a workflow\n",
    "workflow_steps = [\n",
    "    {\n",
    "        'description': 'Search for current Bitcoin price',\n",
    "        'tool': 'Search',\n",
    "        'input': 'current Bitcoin price USD'\n",
    "    },\n",
    "    {\n",
    "        'description': 'Calculate cost of 5 Bitcoin',\n",
    "        'tool': 'Calculator', \n",
    "        'input': '50000 * 5'  # Assuming $50k per Bitcoin\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute workflow\n",
    "workflow_results = workflow_agent.execute_workflow(workflow_steps)\n",
    "print(\"\\nWorkflow Results:\")\n",
    "for step, result in workflow_results.items():\n",
    "    print(f\"{step}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Framework Comparison: LangChain vs AWS Strands\n",
    "\n",
    "While this notebook focuses on LangChain, let's compare it with AWS Strands to understand the trade-offs between comprehensive ecosystems and minimal-code approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 * 4 = 100\n",
      "Strands Response: 25 * 4 = 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AWS Strands with Ollama - Minimal approach\n",
    "# !pip install strands-agents\n",
    "\n",
    "from strands import Agent\n",
    "from strands.models.ollama import OllamaModel\n",
    "\n",
    "# Create an Ollama-backed model\n",
    "ollama_model = OllamaModel(\n",
    "    host=\"http://localhost:11434\",  # Ollama server URL\n",
    "    model_id=\"gemma3\"               # Specify the model you pulled\n",
    ")\n",
    "\n",
    "# Build your agent (minimal code)\n",
    "strands_agent = Agent(model=ollama_model)\n",
    "\n",
    "# Use the agent\n",
    "response = strands_agent(\"What is 25 * 4?\")\n",
    "print(f\"Strands Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Response: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimal LangChain with Ollama (comparable to Strands)\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer briefly: {question}\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is 25 * 4?\"})\n",
    "print(f\"LangChain Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangChain vs AWS Strands Comparison**\n",
    "\n",
    "| Aspect | LangChain | AWS Strands |\n",
    "|--------|-----------|-------------|\n",
    "| **Code Complexity** | Medium (flexible; can be concise) | Low (minimal code) |\n",
    "| **Lines of Code** | 6–20 depending on pattern | 3–8 for simple agent |\n",
    "| **Learning Curve** | Moderate–steep (many concepts) | Gentle (opinionated) |\n",
    "| **Customization** | Highly flexible | Opinionated patterns |\n",
    "| **Ecosystem** | Extensive (1000+ integrations) | AWS-focused |\n",
    "| **Memory Management** | Pluggable memory components | Opinionated defaults |\n",
    "| **Tool Integration** | Manual or schema-based; rich ecosystem | Simplified wiring; AWS-first |\n",
    "| **Multi-Agent** | LangGraph patterns built-in | Built-in patterns |\n",
    "| **Error Handling** | Configurable; retries/callbacks available | Built-in |\n",
    "| **Production Ready** | Portable; requires configuration | AWS-native deployment |\n",
    "| **Local Models** | Full support (Ollama, etc.) | Ollama support |\n",
    "| **Cloud Integration** | Multi-cloud | AWS-optimized |\n",
    "| **Observability** | LangSmith/tracing callbacks | CloudWatch/X-Ray integration |\n",
    "| **Guardrails/Evals** | Prompt/versioning, policy hooks, eval libs | AWS guardrails/services |\n",
    "| **Governance & Security** | BYO controls per platform | IAM/VPC/Data residency via AWS |\n",
    "| **Vendor Lock-in** | Low (portable) | Higher (AWS services) |\n",
    "| **Community** | Large, active | Growing |\n",
    "| **Documentation** | Extensive | Focused |\n",
    "| **Debugging** | Verbose/loggable | Abstracted |\n",
    "| **Performance** | Depends on configuration | Optimized defaults |\n",
    "\n",
    "### **When to Choose Each Framework**\n",
    "\n",
    "**Choose LangChain when:**\n",
    "- You need maximum flexibility and customization\n",
    "- Working with multiple LLM providers or cloud platforms\n",
    "- Building complex, custom agent architectures\n",
    "- Learning agent concepts in depth\n",
    "- Have existing LangChain infrastructure\n",
    "- Need extensive tool ecosystem\n",
    "\n",
    "**Choose AWS Strands when:**\n",
    "- You want rapid prototyping and deployment\n",
    "- Building primarily on AWS infrastructure\n",
    "- Need production-ready agents quickly\n",
    "- Prefer opinionated, best-practice patterns\n",
    "- Want minimal code maintenance\n",
    "- Trust AWS defaults and optimizations\n",
    "\n",
    "### **Code Comparison Summary**\n",
    "\n",
    "**Strands (3 lines):**\n",
    "```python\n",
    "ollama_model = OllamaModel(host=\"http://localhost:11434\", model_id=\"gemma3\")\n",
    "agent = Agent(model=ollama_model)\n",
    "response = agent(\"What is 25 * 4?\")\n",
    "```\n",
    "\n",
    "**LangChain (6 lines, minimal):**\n",
    "```python\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer briefly: {question}\")\n",
    "response = (prompt | llm | StrOutputParser()).invoke({\"question\": \"What is 25 * 4?\"})\n",
    "```\n",
    "\n",
    "Both approaches have merit - LangChain for learning and flexibility, Strands for speed and simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Agent Architecture Patterns\n",
    "\n",
    "This notebook demonstrated key agent patterns using LangChain with local Ollama models:\n",
    "\n",
    "###  **Patterns Covered:**\n",
    "1. **ReAct Pattern** - Reasoning and Acting in loops\n",
    "2. **Plan-and-Execute** - Strategic planning before execution\n",
    "3. **Multi-Agent** - Specialized agents working together\n",
    "4. **Workflow Agent** - Custom orchestration patterns\n",
    "\n",
    "### **Core Capabilities:**\n",
    "- **Tool Integration** - Calculator, search, document retrieval\n",
    "- **Memory Management** - Conversation history and state\n",
    "- **State Persistence** - Stateful tools and workflows\n",
    "- **Error Handling** - Production-ready error management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
